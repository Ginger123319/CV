{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H8WftKW3"
   },
   "outputs": [],
   "source": [
    "# Sample code from the TorchVision 0.3 Object Detection Finetuning Tutorial\n",
    "# http://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from .detection.engine import train_one_epoch, evaluate\n",
    "from .detection import utils\n",
    "from .detection import transforms as T\n",
    "from . import cv2_util\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms, class_name, work_dir):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"image\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"mask\"))))\n",
    "# ------------------------------------------------#\n",
    "        self.labels = list(sorted(os.listdir(os.path.join(work_dir, \"ClassNames\"))))\n",
    "        self.class_name = class_name\n",
    "        self.work_dir = work_dir\n",
    "# ------------------------------------------------#\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"image\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"mask\", self.masks[idx])\n",
    "# ------------------------------------------------#\n",
    "        label_path = os.path.join(self.work_dir, \"ClassNames\", self.labels[idx])\n",
    "        with open(label_path) as f:\n",
    "            content = f.read().split(',')[:-1]\n",
    "            labels = [self.class_name.index(i) for i in content]\n",
    "# ------------------------------------------------#\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "#------------------------------------------------#\n",
    "        mask[mask == 255] = 0\n",
    "# ------------------------------------------------#\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        # labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "# ------------------------------------------------#\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "# ------------------------------------------------#\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes, device, pth_path):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    # model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(pth_path, map_location=device)\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if np.shape(model_dict[k]) == np.shape(v)}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def random_color():\n",
    "    b = random.randint(0, 255)\n",
    "    g = random.randint(0, 255)\n",
    "    r = random.randint(0, 255)\n",
    "\n",
    "    return (b, g, r)\n",
    "\n",
    "\n",
    "def toTensor(img):\n",
    "    assert type(img) == np.ndarray, 'the img type is {}, but ndarry expected'.format(type(img))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = torch.from_numpy(img.transpose((2, 0, 1)))\n",
    "    return img.float().div(255)  # 255也可以改为256\n",
    "\n",
    "\n",
    "def PredictImg(image, model, device):\n",
    "    # img, _ = dataset_test[0]\n",
    "    img = cv2.imread(image)\n",
    "    result = img.copy()\n",
    "    dst = img.copy()\n",
    "    img = toTensor(img)\n",
    "\n",
    "    names = {'0': 'background', '1': 'person'}\n",
    "    # put the model in evaluati\n",
    "    # on mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "\n",
    "    boxes = prediction[0]['boxes']\n",
    "    labels = prediction[0]['labels']\n",
    "    scores = prediction[0]['scores']\n",
    "    masks = prediction[0]['masks']\n",
    "\n",
    "    m_bOK = False;\n",
    "    for idx in range(boxes.shape[0]):\n",
    "        if scores[idx] >= 0.8:\n",
    "            m_bOK = True;\n",
    "            color = random_color()\n",
    "            mask = masks[idx, 0].mul(255).byte().cpu().numpy()\n",
    "            thresh = mask\n",
    "            contours, hierarchy = cv2_util.findContours(\n",
    "                thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE\n",
    "            )\n",
    "            cv2.drawContours(dst, contours, -1, color, -1)\n",
    "\n",
    "            x1, y1, x2, y2 = boxes[idx][0], boxes[idx][1], boxes[idx][2], boxes[idx][3]\n",
    "            name = names.get(str(labels[idx].item()))\n",
    "            cv2.rectangle(result, (x1, y1), (x2, y2), color, thickness=2)\n",
    "            cv2.putText(result, text=name, org=(x1, y1 + 10), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=0.5, thickness=1, lineType=cv2.LINE_AA, color=color)\n",
    "\n",
    "            dst1 = cv2.addWeighted(result, 0.7, dst, 0.3, 0)\n",
    "\n",
    "    if m_bOK:\n",
    "        cv2.imshow('result', dst1)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def train(device, num_classes, val_size, batch_size, lr, optimizer, total_epoch, dataset_path, class_name, model_output_path, pth_path, tensorboard_dir, work_dir):\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    # num_classes = 2 + 1  # 需要修改种类\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = PennFudanDataset(dataset_path, get_transform(train=True), class_name, work_dir)\n",
    "    dataset_test = PennFudanDataset(dataset_path, get_transform(train=False), class_name, work_dir)\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    num_val = int(len(dataset) * val_size)\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[num_val:])  # 训练集张数\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[:num_val])  # 测试集张数\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes, device, pth_path)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)  \n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(params, lr=lr, weight_decay=0.0005)  \n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    # let's train it for n epochs\n",
    "    num_epochs = total_epoch  # 训练次数\n",
    "\n",
    "\n",
    "    # 保存模型训练的tensorboard日志\n",
    "    writer = SummaryWriter(log_dir=tensorboard_dir, flush_secs=60)\n",
    "    # if torch.cuda.is_available():\n",
    "    #     graph_inputs = torch.from_numpy(np.random.rand(1, 3, 512, 512)).type(torch.FloatTensor).cuda()\n",
    "    # else:\n",
    "    #     graph_inputs = torch.from_numpy(np.random.rand(1, 3, 512, 512)).type(torch.FloatTensor)\n",
    "    # graph_inputs = torch.from_numpy(np.random.rand(1, 3, 512, 512)).type(torch.FloatTensor).to(device)\n",
    "    # writer.add_graph(model, graph_inputs, verbose=False)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, 10, writer)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        coco_evaluator = evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    # utils.save_on_master({\n",
    "    #         'model': model_without_ddp.state_dict(),\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'lr_scheduler': lr_scheduler.state_dict()},\n",
    "    #         os.path.join('./', 'model_{}.pth'.format(epoch)))\n",
    "\n",
    "    # utils.save_on_master({'model': model_without_ddp.state_dict()}, os.path.join('./', 'model.pth'))\n",
    "    utils.save_on_master({'model': model.state_dict()}, os.path.join(model_output_path, 'model.pth'))\n",
    "    utils.save_on_master({'model': model.state_dict()}, os.path.join(work_dir, 'pre_training_weights', 'model.pth'))\n",
    "    print(\"That's it!\")\n",
    "    # PredictImg(\"1.jpg\",model,device)\n",
    "    return metric_logger, coco_evaluator\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
